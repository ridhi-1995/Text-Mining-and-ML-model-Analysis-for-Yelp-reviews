{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58e7e11",
   "metadata": {},
   "source": [
    "# Text Mining and ML model Analysis for Yelp reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0a4c7",
   "metadata": {},
   "source": [
    "The objective of this assignment is to scrape a collection of product reviews from a set of web pages, preprocess the data, and evaluate the performance of different classifiers in the context of two related text classification tasks: (i) predicting review sentiment; (ii) predicting review helpfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ed602",
   "metadata": {},
   "source": [
    "### This notebook covers Task 1 - Data Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc2309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries used\n",
    "import requests, urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180c6f1",
   "metadata": {},
   "source": [
    "Create directory for raw data storage, if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6780c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new directory\n",
    "dir_raw = Path(\"raw\")\n",
    "dir_raw.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d660b",
   "metadata": {},
   "source": [
    "Defining years and months we are interested to look into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4e6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The years and months from the review list\n",
    "years = [\"2016\",\"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01819d",
   "metadata": {},
   "source": [
    "We have scrape data from every available page, hence defining a function to find the page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f367e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defination to define page\n",
    "def find_page(page,prefix,href):\n",
    "    suffix = \"0\"+str(page) + \".html\"\n",
    "    url = prefix + href +suffix\n",
    "    response = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42693023",
   "metadata": {},
   "source": [
    "We are interested only in few of the HTML tags, hence defining a function to filter the correct tags with desirable values<br>\n",
    "**Required Fields:**<br>\n",
    "- Main Title<br>\n",
    "- Reiview Title<br>\n",
    "- Review Body<br>\n",
    "- Star Rating<br>\n",
    "- Helpfulness<br>\n",
    "<br>\n",
    "This functions returns a data frame for a particular page.<br>\n",
    "The helpfulness information has been considered as a percentage as it would be more easy to handle it further for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ce09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defination to filter tags\n",
    "def filter_tags(soup):\n",
    "    review=[]\n",
    "    Titles = []\n",
    "    Review_titles=[]\n",
    "    Ratings = []\n",
    "    Helpfulness_infos= []\n",
    "    Review_Bodys =[]\n",
    "    container = soup.find_all('div', class_ = ['review', 'review-alt'])\n",
    "    title= soup.title.text\n",
    "    for con in container:\n",
    "            #assigns main-title to each review\n",
    "            Titles.append(title)\n",
    "            stars = con.find_all(\"img\")\n",
    "            #assign ratings to each review\n",
    "            for star in stars:\n",
    "                Ratings.append(int(star[\"alt\"].split(\"-\")[0]))\n",
    "            #assign body to each review\n",
    "            review_body = con.find_all(\"p\", {\"class\":\"review-body\"})\n",
    "            for result in review_body:\n",
    "                Review_Bodys.append(result.text.strip())\n",
    "            #assign helpfulness information to each review\n",
    "            helpfulness = con.find_all(\"p\", {\"class\":\"metadata\"})[1]\n",
    "            for result in helpfulness:\n",
    "                helpful = result.split(\" \")\n",
    "                num=int(helpful[0])\n",
    "                deno=int(helpful[3])\n",
    "                percent=num*100/deno\n",
    "                Helpfulness_infos.append(percent)\n",
    "            #assign review title to each review\n",
    "            review_title=con.find_all(\"h5\")\n",
    "            for result in review_title:\n",
    "                Review_titles.append(result.text.strip())\n",
    "    #add all these columns to data frame\n",
    "    review= pd.DataFrame({'Main Title': Titles,\n",
    "    'Review Title': Review_titles,\n",
    "    'Rating': Ratings,\n",
    "    'Helpfulness Information': Helpfulness_infos,\n",
    "    'Review Body': Review_Bodys})\n",
    "    return review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817fba2",
   "metadata": {},
   "source": [
    "Defining a function to collect the filter data for each month based on different pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5591ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition to collect data\n",
    "def collect_data(month,year):\n",
    "    review_month = []\n",
    "    href = \"-\" + year + \"-\" + month + \"-\"\n",
    "    suffix = \"01\" + \".html\"\n",
    "    prefix = \"http://mlg.ucd.ie/modules/python/assign2/21201977/reviews\"\n",
    "    #construct main url\n",
    "    url = prefix + href +suffix\n",
    "    response = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    pages= int(soup.find(\"h4\").text.strip().split(\" \")[6])\n",
    "    for page in range(1, pages+1) :\n",
    "        # extract the soup as per each page\n",
    "        soup =find_page(page,prefix,href)\n",
    "        # filter the soup to form review data frame\n",
    "        review_page = filter_tags(soup)\n",
    "        review_month.append(review_page)\n",
    "    reviews_per_month = pd.concat(review_month)\n",
    "    return reviews_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745acd4",
   "metadata": {},
   "source": [
    "Concatenate dataframe for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7274ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Data for:  2016 jan\n",
      "Filtering Data for:  2016 feb\n",
      "Filtering Data for:  2016 mar\n",
      "Filtering Data for:  2016 apr\n",
      "Filtering Data for:  2016 may\n",
      "Filtering Data for:  2016 jun\n",
      "Filtering Data for:  2016 jul\n",
      "Filtering Data for:  2016 aug\n",
      "Filtering Data for:  2016 sep\n",
      "Filtering Data for:  2016 oct\n",
      "Filtering Data for:  2016 nov\n",
      "Filtering Data for:  2016 dec\n",
      "Filtering Data for:  2017 jan\n",
      "Filtering Data for:  2017 feb\n",
      "Filtering Data for:  2017 mar\n",
      "Filtering Data for:  2017 apr\n",
      "Filtering Data for:  2017 may\n",
      "Filtering Data for:  2017 jun\n",
      "Filtering Data for:  2017 jul\n",
      "Filtering Data for:  2017 aug\n",
      "Filtering Data for:  2017 sep\n",
      "Filtering Data for:  2017 oct\n",
      "Filtering Data for:  2017 nov\n",
      "Filtering Data for:  2017 dec\n",
      "Filtering Data for:  2018 jan\n",
      "Filtering Data for:  2018 feb\n",
      "Filtering Data for:  2018 mar\n",
      "Filtering Data for:  2018 apr\n",
      "Filtering Data for:  2018 may\n",
      "Filtering Data for:  2018 jun\n",
      "Filtering Data for:  2018 jul\n",
      "Filtering Data for:  2018 aug\n",
      "Filtering Data for:  2018 sep\n",
      "Filtering Data for:  2018 oct\n",
      "Filtering Data for:  2018 nov\n",
      "Filtering Data for:  2018 dec\n",
      "Filtering Data for:  2019 jan\n",
      "Filtering Data for:  2019 feb\n",
      "Filtering Data for:  2019 mar\n",
      "Filtering Data for:  2019 apr\n",
      "Filtering Data for:  2019 may\n",
      "Filtering Data for:  2019 jun\n",
      "Filtering Data for:  2019 jul\n",
      "Filtering Data for:  2019 aug\n",
      "Filtering Data for:  2019 sep\n",
      "Filtering Data for:  2019 oct\n",
      "Filtering Data for:  2019 nov\n",
      "Filtering Data for:  2019 dec\n",
      "Filtering Data for:  2020 jan\n",
      "Filtering Data for:  2020 feb\n",
      "Filtering Data for:  2020 mar\n",
      "Filtering Data for:  2020 apr\n",
      "Filtering Data for:  2020 may\n",
      "Filtering Data for:  2020 jun\n",
      "Filtering Data for:  2020 jul\n",
      "Filtering Data for:  2020 aug\n",
      "Filtering Data for:  2020 sep\n",
      "Filtering Data for:  2020 oct\n",
      "Filtering Data for:  2020 nov\n",
      "Filtering Data for:  2020 dec\n",
      "Filtering Data for:  2021 jan\n",
      "Filtering Data for:  2021 feb\n",
      "Filtering Data for:  2021 mar\n",
      "Filtering Data for:  2021 apr\n",
      "Filtering Data for:  2021 may\n",
      "Filtering Data for:  2021 jun\n",
      "Filtering Data for:  2021 jul\n",
      "Filtering Data for:  2021 aug\n",
      "Filtering Data for:  2021 sep\n",
      "Filtering Data for:  2021 oct\n",
      "Filtering Data for:  2021 nov\n",
      "Filtering Data for:  2021 dec\n"
     ]
    }
   ],
   "source": [
    "review_year =[]\n",
    "review=[]\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        print(\"Filtering Data for: \",year,month)\n",
    "        review = collect_data(month, year)\n",
    "        review_year.append(review)\n",
    "    # concatenate each month's review for every year\n",
    "    reviews_per_year=pd.concat(review_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbc06",
   "metadata": {},
   "source": [
    "Saving this filtered data as .csv file for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data in .csv format\n",
    "filePath = dir_raw/\"reviwes.csv\"\n",
    "reviews_per_year.to_csv(filePath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
